{
 "metadata": {
  "name": "",
  "signature": "sha256:e8965798c97ce588e103579d3769a1080a1b9ed93f45a241ab96034e5331061d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A tutorial on logistic models (for predicting risk of software changes)\n",
      "======\n",
      "Full data is at:\n",
      "  http://mockus.org/riskTut.zip and\n",
      "\n",
      "* Version 3.2.1b\n",
      "* Except where otherwise noted, the content on this site is licensed under a Creative Commons Attribution 3.0 License.\n",
      " http://creativecommons.org/licenses/by/3.0/"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# For nicer printing\n",
      "options(digits=2);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Read in the data\n",
      "risk <- read.table(\"risk1.paper\",header=T);\n",
      "\n",
      "#It typiaclly is a no-brainer for operational data: log-transform ALL the numeric variables\n",
      "vars <- cbind (risk$isBad, log(risk$NS),log(risk$NM),log(risk$NF),log(risk$NLOGIN),\n",
      "           log(risk$NMR),log(risk$ND),log(risk$LA+1),log(risk$LD+1),\n",
      "           log(risk$LinesUnchanged+1),log(risk$to-risk$from+1),risk$logEXP,risk$logREXP,\n",
      "           risk$logSEXP,risk$FIX);\n",
      "dimnames(vars)[[2]] <- c(\"isBad\",\"lNS\", \"lNM\", \"lNF\", \"lNLgn\",  \"lNMR\", \"lND\",\n",
      "                         \"lLA\", \"lLD\", \"lLOC\", \"lINT\", \"lEXP\", \"lREXP\", \"lSEXP\",\"FIX\"); \n",
      "\n",
      "#R likes data.frame (as pythoin numpy): a matrix with (potentially) different types in each column\n",
      "data <- data.frame(vars);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Variable names as in the paper: http://mockus.org/papers/bltj13.pdf\n",
      "Audris Mockus and David M. Weiss. Predicting risk of software changes. Bell Labs Technical Journal, 5(2):169-180, April-June 2000. \n",
      "\n",
      "What hypothesis is associated with each variable?\n",
      "\n",
      "\n",
      "| Type       | Name   | Description                             |\n",
      "|------------+--------+-----------------------------------------|\n",
      "| Response   | isBad  | Did MR cause patch to fail?             |\n",
      "|            | NS     | Number of subsystems touched.           |\n",
      "| Diffusion  | NM     | Number of modules touched.              |\n",
      "|            | NF     | Number of files touched.                |\n",
      "|            | NLOGIN | Number of developers involved.          |\n",
      "|            | LA     | LOC added.                              |\n",
      "| Size       | LD     | LOC deleted.                            |\n",
      "|            | LT     | LOC in the files touched by the change. |\n",
      "| Diffusion  | NMR    | Number of MRs.                          |\n",
      "| and size   | ND     | Number of deltas.                       |\n",
      "| Interval   | INT    | Time between the last and first delta.  |\n",
      "| Purpose    | FIX    | Fix of a defect found in the field.     |\n",
      "|            | EXP    | Developer experience.                   |\n",
      "| Experience | REXP   | Recent developer experience.            |\n",
      "|            | SEXP   | Developer experience on a subsystem.    |\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#now explore variables\n",
      "summary(data);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "     isBad           lNS            lNM           lNF          lNLgn    \n",
        " Min.   :0.00   Min.   :0.00   Min.   :0.0   Min.   :0.0   Min.   :0.0  \n",
        " 1st Qu.:0.00   1st Qu.:0.00   1st Qu.:0.0   1st Qu.:0.0   1st Qu.:0.0  \n",
        " Median :0.00   Median :0.00   Median :0.0   Median :0.7   Median :0.0  \n",
        " Mean   :0.02   Mean   :0.29   Mean   :0.6   Mean   :1.0   Mean   :0.3  \n",
        " 3rd Qu.:0.00   3rd Qu.:0.69   3rd Qu.:1.1   3rd Qu.:1.6   3rd Qu.:0.7  \n",
        " Max.   :1.00   Max.   :2.89   Max.   :6.5   Max.   :7.7   Max.   :3.6  \n",
        "      lNMR          lND           lLA            lLD            lLOC     \n",
        " Min.   :0.0   Min.   :0.0   Min.   : 0.0   Min.   : 0.0   Min.   : 0.0  \n",
        " 1st Qu.:0.0   1st Qu.:0.7   1st Qu.: 2.6   1st Qu.: 0.7   1st Qu.: 7.6  \n",
        " Median :0.7   Median :1.6   Median : 3.9   Median : 2.3   Median : 8.9  \n",
        " Mean   :0.9   Mean   :1.8   Mean   : 4.1   Mean   : 2.7   Mean   : 9.0  \n",
        " 3rd Qu.:1.4   3rd Qu.:2.7   3rd Qu.: 5.4   3rd Qu.: 4.1   3rd Qu.:10.3  \n",
        " Max.   :5.7   Max.   :7.9   Max.   :14.4   Max.   :14.3   Max.   :16.9  \n",
        "      lINT           lEXP          lREXP          lSEXP           FIX      \n",
        " Min.   : 0.0   Min.   : 0.4   Min.   : 0.4   Min.   : 0.0   Min.   :0.00  \n",
        " 1st Qu.: 0.0   1st Qu.: 5.6   1st Qu.: 4.9   1st Qu.: 4.0   1st Qu.:0.00  \n",
        " Median : 0.0   Median : 6.6   Median : 5.7   Median : 6.0   Median :0.00  \n",
        " Mean   : 4.8   Mean   : 6.4   Mean   : 5.6   Mean   : 5.5   Mean   :0.41  \n",
        " 3rd Qu.:16.1   3rd Qu.: 7.4   3rd Qu.: 6.3   3rd Qu.: 7.1   3rd Qu.:1.00  \n",
        " Max.   :19.7   Max.   :12.0   Max.   :11.0   Max.   :12.0   Max.   :1.00  "
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Interpret  basic summaries"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cor(vars);# OK for normal distrubution\n",
      "cor(vars,method=\"spearman\"); #OK for any: uses ranks"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "        isBad    lNS    lNM     lNF  lNLgn   lNMR    lND     lLA     lLD   lLOC\n",
        "isBad  1.0000  0.128  0.137  0.1308  0.124  0.142  0.136  0.1280  0.1211  0.118\n",
        "lNS    0.1283  1.000  0.824  0.7360  0.756  0.772  0.674  0.5836  0.5536  0.548\n",
        "lNM    0.1368  0.824  1.000  0.9027  0.757  0.827  0.817  0.7103  0.6630  0.649\n",
        "lNF    0.1308  0.736  0.903  1.0000  0.749  0.850  0.908  0.8024  0.7435  0.688\n",
        "lNLgn  0.1238  0.756  0.757  0.7491  1.000  0.808  0.733  0.6505  0.6338  0.601\n",
        "lNMR   0.1420  0.772  0.827  0.8504  0.808  1.000  0.877  0.7776  0.7506  0.720\n",
        "lND    0.1355  0.674  0.817  0.9078  0.733  0.877  1.000  0.8863  0.8557  0.792\n",
        "lLA    0.1280  0.584  0.710  0.8024  0.651  0.778  0.886  1.0000  0.8269  0.724\n",
        "lLD    0.1211  0.554  0.663  0.7435  0.634  0.751  0.856  0.8269  1.0000  0.698\n",
        "lLOC   0.1179  0.548  0.649  0.6882  0.601  0.720  0.792  0.7243  0.6976  1.000\n",
        "lINT   0.0992  0.457  0.533  0.5735  0.592  0.644  0.635  0.5760  0.5616  0.517\n",
        "lEXP  -0.0333 -0.022  0.023  0.0046 -0.080 -0.053 -0.031 -0.0123 -0.0200 -0.043\n",
        "lREXP -0.0318 -0.020  0.034  0.0174 -0.082 -0.050 -0.014 -0.0035 -0.0027 -0.035\n",
        "lSEXP -0.0739 -0.616 -0.445 -0.3916 -0.493 -0.455 -0.359 -0.3095 -0.2836 -0.275\n",
        "FIX   -0.0029 -0.179 -0.210 -0.2463 -0.231 -0.240 -0.227 -0.1999 -0.2210 -0.151\n",
        "        lINT    lEXP   lREXP  lSEXP     FIX\n",
        "isBad  0.099 -0.0333 -0.0318 -0.074 -0.0029\n",
        "lNS    0.457 -0.0223 -0.0202 -0.616 -0.1794\n",
        "lNM    0.533  0.0235  0.0335 -0.445 -0.2100\n",
        "lNF    0.574  0.0046  0.0174 -0.392 -0.2463\n",
        "lNLgn  0.592 -0.0796 -0.0824 -0.493 -0.2306\n",
        "lNMR   0.644 -0.0526 -0.0496 -0.455 -0.2403\n",
        "lND    0.635 -0.0309 -0.0144 -0.359 -0.2269\n",
        "lLA    0.576 -0.0123 -0.0035 -0.310 -0.1999\n",
        "lLD    0.562 -0.0200 -0.0027 -0.284 -0.2210\n",
        "lLOC   0.517 -0.0434 -0.0351 -0.275 -0.1510\n",
        "lINT   1.000 -0.1015 -0.0975 -0.281 -0.1918\n",
        "lEXP  -0.101  1.0000  0.9741  0.363 -0.0418\n",
        "lREXP -0.098  0.9741  1.0000  0.375 -0.0518\n",
        "lSEXP -0.281  0.3634  0.3751  1.000  0.0948\n",
        "FIX   -0.192 -0.0418 -0.0518  0.095  1.0000"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "        isBad    lNS     lNM     lNF  lNLgn   lNMR    lND    lLA    lLD   lLOC\n",
        "isBad  1.0000  0.100  0.1084  0.1097  0.100  0.113  0.113  0.115  0.105  0.106\n",
        "lNS    0.1004  1.000  0.7551  0.6501  0.644  0.652  0.577  0.512  0.477  0.489\n",
        "lNM    0.1084  0.755  1.0000  0.8561  0.646  0.743  0.744  0.653  0.591  0.605\n",
        "lNF    0.1097  0.650  0.8561  1.0000  0.635  0.770  0.862  0.757  0.672  0.647\n",
        "lNLgn  0.0999  0.644  0.6457  0.6352  1.000  0.696  0.638  0.577  0.558  0.551\n",
        "lNMR   0.1130  0.652  0.7432  0.7702  0.696  1.000  0.835  0.736  0.696  0.690\n",
        "lND    0.1128  0.577  0.7438  0.8615  0.638  0.835  1.000  0.868  0.819  0.772\n",
        "lLA    0.1146  0.512  0.6532  0.7571  0.577  0.736  0.868  1.000  0.776  0.707\n",
        "lLD    0.1052  0.477  0.5907  0.6716  0.558  0.696  0.819  0.776  1.000  0.664\n",
        "lLOC   0.1057  0.489  0.6050  0.6472  0.551  0.690  0.772  0.707  0.664  1.000\n",
        "lINT   0.1039  0.436  0.5100  0.5443  0.620  0.639  0.609  0.558  0.539  0.517\n",
        "lEXP  -0.0374 -0.027 -0.0092 -0.0160 -0.097 -0.059 -0.054 -0.031 -0.045 -0.059\n",
        "lREXP -0.0336 -0.019  0.0063  0.0031 -0.091 -0.048 -0.031 -0.018 -0.019 -0.046\n",
        "lSEXP -0.0661 -0.595 -0.3879 -0.3268 -0.447 -0.382 -0.303 -0.262 -0.245 -0.258\n",
        "FIX   -0.0029 -0.149 -0.1735 -0.2093 -0.213 -0.202 -0.193 -0.172 -0.198 -0.144\n",
        "       lINT    lEXP   lREXP  lSEXP     FIX\n",
        "isBad  0.10 -0.0374 -0.0336 -0.066 -0.0029\n",
        "lNS    0.44 -0.0272 -0.0191 -0.595 -0.1487\n",
        "lNM    0.51 -0.0092  0.0063 -0.388 -0.1735\n",
        "lNF    0.54 -0.0160  0.0031 -0.327 -0.2093\n",
        "lNLgn  0.62 -0.0968 -0.0909 -0.447 -0.2134\n",
        "lNMR   0.64 -0.0593 -0.0481 -0.382 -0.2024\n",
        "lND    0.61 -0.0536 -0.0311 -0.303 -0.1928\n",
        "lLA    0.56 -0.0314 -0.0182 -0.262 -0.1723\n",
        "lLD    0.54 -0.0447 -0.0188 -0.245 -0.1982\n",
        "lLOC   0.52 -0.0591 -0.0457 -0.258 -0.1443\n",
        "lINT   1.00 -0.1314 -0.1239 -0.285 -0.1979\n",
        "lEXP  -0.13  1.0000  0.9595  0.400 -0.0382\n",
        "lREXP -0.12  0.9595  1.0000  0.409 -0.0512\n",
        "lSEXP -0.29  0.4004  0.4089  1.000  0.0879\n",
        "FIX   -0.20 -0.0382 -0.0512  0.088  1.0000"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Interpret correlations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, just show top correlations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Define a function\n",
      "hiCor <- function(x, level){\n",
      "  res <- cor(x,method=\"spearman\");\n",
      "  res1 <- res; res1[res<0] <- -res[res < 0];\n",
      "  for (i in 1:dim(x)[2]){\n",
      "    res1[i,i] <- 0;\n",
      "  }\n",
      "  sel <- apply(res1,1,max) > level;\n",
      "  res[sel,sel];\n",
      "}\n",
      "hiCor(data,.7)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "         lNS     lNM     lNF   lNMR    lND    lLA    lLD   lLOC    lEXP   lREXP\n",
        "lNS    1.000  0.7551  0.6501  0.652  0.577  0.512  0.477  0.489 -0.0272 -0.0191\n",
        "lNM    0.755  1.0000  0.8561  0.743  0.744  0.653  0.591  0.605 -0.0092  0.0063\n",
        "lNF    0.650  0.8561  1.0000  0.770  0.862  0.757  0.672  0.647 -0.0160  0.0031\n",
        "lNMR   0.652  0.7432  0.7702  1.000  0.835  0.736  0.696  0.690 -0.0593 -0.0481\n",
        "lND    0.577  0.7438  0.8615  0.835  1.000  0.868  0.819  0.772 -0.0536 -0.0311\n",
        "lLA    0.512  0.6532  0.7571  0.736  0.868  1.000  0.776  0.707 -0.0314 -0.0182\n",
        "lLD    0.477  0.5907  0.6716  0.696  0.819  0.776  1.000  0.664 -0.0447 -0.0188\n",
        "lLOC   0.489  0.6050  0.6472  0.690  0.772  0.707  0.664  1.000 -0.0591 -0.0457\n",
        "lEXP  -0.027 -0.0092 -0.0160 -0.059 -0.054 -0.031 -0.045 -0.059  1.0000  0.9595\n",
        "lREXP -0.019  0.0063  0.0031 -0.048 -0.031 -0.018 -0.019 -0.046  0.9595  1.0000"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## So much correlation is typical in SE\n",
      "* How to select an orthogonal subset of predictors?\n",
      "\n",
      "Lets take a look at principal components"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(1:15,cumsum(prcomp(vars, retx=F,scale=T)$sdev^2)/sum(prcomp(vars, retx=F,scale=T)$sdev^2),ylim=c(0,1),xlab=\"Number of coponents\",ylab=\"Fraction of variance\");\n",
      "res<-prcomp(vars, retx=F,scale=T)$rotation[,1:5];\n",
      "resAbs <- res;\n",
      "resAbs[res<0] <- -res[res<0];\n",
      "for (i in 1:5)\n",
      "  print(t(res[resAbs[,i]>.3,i,drop=FALSE]));"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "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"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "     lNM  lNF lNLgn lNMR  lND  lLA lLD\n",
        "PC1 0.32 0.33  0.31 0.34 0.34 0.31 0.3\n",
        "     lEXP lREXP lSEXP\n",
        "PC2 -0.65 -0.66 -0.33\n",
        "    isBad FIX\n",
        "PC3  0.84 0.5\n",
        "      lNS lSEXP\n",
        "PC4 -0.46  0.54\n",
        "    isBad   FIX\n",
        "PC5   0.5 -0.84\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Regress each predictor on the remaining predictors\n",
      "eliminate with the highest adjR^2"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res <- c();\n",
      "vnam <- names(data);\n",
      "for (i in 2:dim(data)[2]){\n",
      "  fmla <- as.formula(paste(vnam[i],paste(vnam[-c(1,i)],collapse=\"+\"),sep=\"~\"));\n",
      "  res <- rbind(res,c(i,round(summary(lm(fmla,data=data))$r.squared,2)));\n",
      "}\n",
      "row.names(res) <- vnam[res[,1]];\n",
      "res[order(-res[,2]),];"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "      [,1] [,2]\n",
        "lEXP    12 0.95\n",
        "lREXP   13 0.95\n",
        "lND      7 0.94\n",
        "lNF      4 0.91\n",
        "lNM      3 0.87\n",
        "lNMR     6 0.86\n",
        "lLA      8 0.81\n",
        "lNS      2 0.80\n",
        "lLD      9 0.76\n",
        "lNLgn    5 0.72\n",
        "lLOC    10 0.65\n",
        "lSEXP   14 0.52\n",
        "lINT    11 0.47\n",
        "FIX     15 0.08"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Finally select the model\n",
      "  Note, that exploring only the predictor space is not leading to\n",
      "     multiple comparisons issue!\n",
      "\n",
      "  * replace lSEXP by lEXP: simpler \n",
      "  * why lLOC, lLA, lLD, lND, lNlgn: keep lND as most stable, keep lLOC?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fmla ~ isBad ~ lNS+lLA+FIX+lLOC+lINT+lEXP"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "fmla ~ isBad ~ lNS + lLA + FIX + lLOC + lINT + lEXP\n",
        "<environment: 0x2c828b0>"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* We'll need to do prediction, so drop last few years"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data1 <- data;\n",
      "data1$from <- risk$from/3600/24/365.25+1970;\n",
      "dataFit <- data1[data1$from<=1997,]; #80%\n",
      "dataTest <- data1[data1$from>1997,]; #20%"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mod <- glm(isBad ~ lNS+lLA+FIX+lLOC+lINT+lEXP,family=binomial,data=dataFit);\n",
      "summary(mod); "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "\n",
        "Call:\n",
        "glm(formula = isBad ~ lNS + lLA + FIX + lLOC + lINT + lEXP, family = binomial, \n",
        "    data = dataFit)\n",
        "\n",
        "Deviance Residuals: \n",
        "   Min      1Q  Median      3Q     Max  \n",
        "-0.842  -0.225  -0.169  -0.132   3.327  \n",
        "\n",
        "Coefficients:\n",
        "            Estimate Std. Error z value Pr(>|z|)    \n",
        "(Intercept) -5.79427    0.45010  -12.87  < 2e-16 ***\n",
        "lNS          0.35650    0.10268    3.47  0.00052 ***\n",
        "lLA          0.17910    0.04751    3.77  0.00016 ***\n",
        "FIX          0.53314    0.12943    4.12  3.8e-05 ***\n",
        "lLOC         0.13590    0.04948    2.75  0.00603 ** \n",
        "lINT         0.01788    0.00925    1.93  0.05322 .  \n",
        "lEXP        -0.10757    0.03829   -2.81  0.00497 ** \n",
        "---\n",
        "Signif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n",
        "\n",
        "(Dispersion parameter for binomial family taken to be 1)\n",
        "\n",
        "    Null deviance: 3011.5  on 13480  degrees of freedom\n",
        "Residual deviance: 2739.0  on 13474  degrees of freedom\n",
        "AIC: 2753\n",
        "\n",
        "Number of Fisher Scoring iterations: 7\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note AIC: 2753\n",
      "\n",
      "threfore lINT probably not important"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Good to order by variance explained (Anova Deviance/Df)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "anova(mod, test=\"Chi\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "Analysis of Deviance Table\n",
        "\n",
        "Model: binomial, link: logit\n",
        "\n",
        "Response: isBad\n",
        "\n",
        "Terms added sequentially (first to last)\n",
        "\n",
        "\n",
        "     Df Deviance Resid. Df Resid. Dev Pr(>Chi)    \n",
        "NULL                 13480       3012             \n",
        "lNS   1    166.1     13479       2845   <2e-16 ***\n",
        "lLA   1     65.1     13478       2780    7e-16 ***\n",
        "FIX   1     17.4     13477       2763    3e-05 ***\n",
        "lLOC  1     10.7     13476       2752   0.0011 ** \n",
        "lINT  1      5.5     13475       2747   0.0193 *  \n",
        "lEXP  1      7.7     13474       2739   0.0054 ** \n",
        "---\n",
        "Signif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A similar picture, lINT does not explain as much as others"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ERROR",
       "evalue": "Error in parse(text = x, srcfile = src): <text>:1:3: unexpected symbol\n1: A similar\n      ^\n",
       "output_type": "pyerr",
       "traceback": [
        "Error in parse(text = x, srcfile = src): <text>:1:3: unexpected symbol\n1: A similar\n      ^\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mod <- glm(isBad ~ lNS+lND+FIX+lEXP+lINT,family=binomial,data=dataFit);\n",
      "summary(mod);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "\n",
        "Call:\n",
        "glm(formula = isBad ~ lNS + lND + FIX + lEXP + lINT, family = binomial, \n",
        "    data = dataFit)\n",
        "\n",
        "Deviance Residuals: \n",
        "   Min      1Q  Median      3Q     Max  \n",
        "-0.851  -0.223  -0.170  -0.135   3.235  \n",
        "\n",
        "Coefficients:\n",
        "            Estimate Std. Error z value Pr(>|z|)    \n",
        "(Intercept) -4.48210    0.28367  -15.80  < 2e-16 ***\n",
        "lNS          0.31994    0.11021    2.90   0.0037 ** \n",
        "lND          0.36503    0.05354    6.82  9.2e-12 ***\n",
        "FIX          0.52993    0.12873    4.12  3.8e-05 ***\n",
        "lEXP        -0.10508    0.03799   -2.77   0.0057 ** \n",
        "lINT         0.01847    0.00941    1.96   0.0496 *  \n",
        "---\n",
        "Signif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1\n",
        "\n",
        "(Dispersion parameter for binomial family taken to be 1)\n",
        "\n",
        "    Null deviance: 3011.5  on 13480  degrees of freedom\n",
        "Residual deviance: 2750.4  on 13475  degrees of freedom\n",
        "AIC: 2762\n",
        "\n",
        "Number of Fisher Scoring iterations: 7\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Slightly higher AIC: 2762, but a simpler model\n",
      "   * don't chase the best fit, as it leads to overfitting"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "anova(mod, test=\"Chi\");#it is good to order predictors by deviance explained"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "Analysis of Deviance Table\n",
        "\n",
        "Model: binomial, link: logit\n",
        "\n",
        "Response: isBad\n",
        "\n",
        "Terms added sequentially (first to last)\n",
        "\n",
        "\n",
        "     Df Deviance Resid. Df Resid. Dev Pr(>Chi)    \n",
        "NULL                 13480       3012             \n",
        "lNS   1    166.1     13479       2845  < 2e-16 ***\n",
        "lND   1     64.3     13478       2781  1.1e-15 ***\n",
        "FIX   1     17.7     13477       2763  2.6e-05 ***\n",
        "lEXP  1      9.3     13476       2754   0.0024 ** \n",
        "lINT  1      3.8     13475       2750   0.0502 .  \n",
        "---\n",
        "Signif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Note: use test=\"F\" for linear models (lm)\n",
      "* Note: R's anova order matters in variance explained, as\n",
      "  each SS is based on the residuals from predictors going before it\n",
      "* Alternative sum of squares for ANOVA are obtained via drop1\n",
      "*  In this case it is based on the residuals of remaining predictors (not\n",
      "  just of preceeding predictors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "drop1(mod, test=\"Chi\");"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "Single term deletions\n",
        "\n",
        "Model:\n",
        "isBad ~ lNS + lND + FIX + lEXP + lINT\n",
        "       Df Deviance  AIC  LRT Pr(>Chi)    \n",
        "<none>        2750 2762                  \n",
        "lNS     1     2759 2769  8.6   0.0033 ** \n",
        "lND     1     2795 2805 44.4  2.6e-11 ***\n",
        "FIX     1     2767 2777 16.8  4.1e-05 ***\n",
        "lEXP    1     2758 2768  7.5   0.0061 ** \n",
        "lINT    1     2754 2764  3.8   0.0502 .  \n",
        "---\n",
        "Signif. codes:  0 \u2018***\u2019 0.001 \u2018**\u2019 0.01 \u2018*\u2019 0.05 \u2018.\u2019 0.1 \u2018 \u2019 1"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Variance inflation factor\n",
      " http://en.wikipedia.org/wiki/Variance_inflation_factor"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "library(car)\n",
      "vif(mod);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        " lNS  lND  FIX lEXP lINT \n",
        " 2.3  3.1  1.2  1.0  1.9 "
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Is less than 5 (max is 3.1 for lND), but lower would be better"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise: other diagnostics\n",
      "* is model stable if some data is dropped?\n",
      "* what if we change data as in GDF?\n",
      "* is there a time trend \n",
      "    isBad ~ lNS+lND+FIX+lEXP+lINT+from\n",
      "* independece (residuals)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now interpret the model\n",
      "* What do estimated coefficients mean?\n",
      "* Increase NS from 1 to 2, but other predictors matter\n",
      "* threfore pick values that are reasonable or a median"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try <- dataFit[1:2,];\n",
      "for (i in dim(dataFit)[2])\n",
      "  try[,i] <- median(dataFit[,i]);\n",
      "try[1,\"lNS\"] <- 0;\n",
      "try[2,\"lNS\"] <- log(2);\n",
      "res <- 1/(1+exp(-predict(mod,try)));\n",
      "res[2]*(1-res[1])/res[1]/(1-res[2]);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "  2 \n",
        "1.9 "
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### The answer is how much the risk increases if number of subsystems touched is increased from 1 to 2\n",
      "  * First ratio of risk then \n",
      "  * ratio of odds"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Do prediction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicted <- 1/(1+exp(-predict(mod,dataTest)));\n",
      "tapply(predicted, dataTest$isBad, mean)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "    0     1 \n",
        "0.022 0.060 "
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Good to know\n",
      "* predicted probability is almost three times higher for the MRs that break patches"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Traditional performance:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for (cutof in c(.01, .015, .02, .03, .04, .045,.1)){\n",
      "  res <- table(predicted>cutof, dataTest$isBad);\n",
      "  type1 <- res[2,1]/(res[1,1]+res[2,1]);\n",
      "  type2 <- res[1,2]/(res[1,2]+res[2,2]);\n",
      "  recall <- 1 - type2;\n",
      "  precision <- res[2,2]/(res[2,1]+res[2,2]);\n",
      "  print (c(cutof,type1,type2,recall,precision));\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1] 0.010 0.661 0.120 0.880 0.017\n",
        "[1] 0.015 0.435 0.260 0.740 0.021\n",
        "[1] 0.020 0.307 0.420 0.580 0.024\n",
        "[1] 0.030 0.177 0.440 0.560 0.039\n",
        "[1] 0.040 0.126 0.520 0.480 0.046\n",
        "[1] 0.045 0.109 0.540 0.460 0.051\n",
        "[1] 0.100 0.026 0.740 0.260 0.114\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### A single number such as ROC typically makes little sense:\n",
      "* in this case the primary concern is decent recall"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exercise:\n",
      "* compare to a random predictor\n",
      "* compare to a simple predictor, e.g, more than one subsystem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Try other prediction methods"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "library(rpart);\n",
      "\n",
      "fmla <- isBad ~ lNS+lND+FIX+lINT+lEXP+lLA;\n",
      "er <- c();\n",
      "for (w in c(20,40,60,80, 100, 110)){ \n",
      "  cart.fit <- rpart(fmla,data=dataFit,method=\"class\",weights=as.numeric(dataFit$isBad)*w+1);\n",
      "  cart.pred <- predict(cart.fit,newdata=dataTest,type=\"class\");\n",
      "  res <- table(cart.pred, dataTest$isBad);\n",
      "  type1 <- res[2,1]/(res[1,1]+res[2,1]);\n",
      "  type2 <- res[1,2]/(res[1,2]+res[2,2]);\n",
      "  recall <- 1 - type2;\n",
      "  precision <- res[2,2]/(res[2,1]+res[2,2]);\n",
      "  er <- rbind (er, c(w,type1,type2,recall,precision));\n",
      "}\n",
      "er "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "     [,1]  [,2] [,3] [,4]  [,5]\n",
        "[1,]   20 0.087 0.54 0.46 0.064\n",
        "[2,]   40 0.363 0.34 0.66 0.023\n",
        "[3,]   60 0.481 0.26 0.74 0.019\n",
        "[4,]   80 0.553 0.20 0.80 0.018\n",
        "[5,]  100 0.645 0.14 0.86 0.017\n",
        "[6,]  110 0.645 0.14 0.86 0.017"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Slightly worse than the regression model"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}